# -*- coding: utf-8 -*-
import os
import sys

# Set up Hadoop environment variables
os.environ['HADOOP_HOME'] = "C:\\hadoop"
os.environ['SPARK_LOCAL_DIRS'] = 'C:\\temp'
os.environ['JAVA_HOME'] = "C:\\Program Files\\Java\\jdk-17"  # Adjust this path to match your Java installation

# Add hadoop binary to system path
if sys.platform.startswith('win'):
    os.environ['PATH'] = os.environ['PATH'] + ';' + os.environ['HADOOP_HOME'] + '\\bin'

"""BigDataProj.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kCNOf5B4SDiWkDVD2lyym3weozlEgidt

1.Data Loading
"""

# 1. Imports and setup
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, explode, isnan, isnull, count, when, udf
from pyspark.sql.types import StringType
from pyspark.sql.window import Window
from pyspark.sql.functions import countDistinct
from pyspark.ml.feature import RegexTokenizer

import re
import string
import emoji

# Initialize Spark session with simplified configuration
spark = SparkSession.builder \
    .appName("TwitterSentimentAnalysis") \
    .config("spark.driver.memory", "4g") \
    .config("spark.executor.memory", "4g") \
    .config("spark.driver.host", "localhost") \
    .master("local[*]") \
    .getOrCreate()

# Suppress INFO and WARN messages
spark.sparkContext.setLogLevel("ERROR")

# Get the current directory
current_dir = os.path.dirname(os.path.abspath(__file__))

print("Loading data files...")

# Load the CSV files with column names and UTF-8 encoding
try:
    columns = ['id', 'entity', 'sentiment', 'tweet']
    
    # Load training data
    train_file = os.path.join(current_dir, 'twitter_training.csv')
    if not os.path.exists(train_file):
        raise FileNotFoundError(f"Training file not found: {train_file}")
    
    train_df = spark.read.csv(
        train_file,
        header=False,
        inferSchema=True
    ).toDF(*columns)
    print("Training data loaded successfully")

    # Load validation data
    test_file = os.path.join(current_dir, 'twitter_validation.csv')
    if not os.path.exists(test_file):
        raise FileNotFoundError(f"Validation file not found: {test_file}")
    
    test_df = spark.read.csv(
        test_file,
        header=False,
        inferSchema=True
    ).toDF(*columns)
    print("Validation data loaded successfully")

    # Select only 'sentiment' and 'tweet' columns
    train_df = train_df.select("sentiment", "tweet")
    test_df = test_df.select("sentiment", "tweet")

    # Combine train and test DataFrames
    tweets_df = train_df.union(test_df)

    print("\nDataset loaded successfully!")
    print("\nSample Tweets:")
    tweets_df.show(5, truncate=False)

except Exception as e:
    print(f"Error loading or processing data: {str(e)}")
    spark.stop()
    sys.exit(1)

# Count missing values in each column
tweets_df.select([
    count(when(isnull(c) | isnan(c), c)).alias(c) for c in tweets_df.columns
]).show()

# Count duplicate rows
duplicate_count = tweets_df.groupBy("sentiment", "tweet").count().filter("count > 1").count()
print("Duplicate Tweets:", duplicate_count)

# Drop rows with nulls
tweets_df = tweets_df.dropna()

# Drop exact duplicate rows
tweets_df = tweets_df.dropDuplicates()

# Drop tweets with conflicting sentiments (same tweet, different sentiment)
from pyspark.sql.window import Window
from pyspark.sql.functions import countDistinct

conflict_check = tweets_df.groupBy("tweet").agg(countDistinct("sentiment").alias("sentiment_count"))
conflict_tweets = conflict_check.filter("sentiment_count > 1")

# Join to filter out conflicting tweets
tweets_df = tweets_df.join(conflict_tweets, on="tweet", how="left_anti")

# Show cleaned data
tweets_df.show(5)

"""2.Data Preprocessing

2.1 Data Cleaning
"""

# 2. Data Cleaning UDF
def pre_process(text: str) -> str:
    if text is None:
        return ""

    # Lowercase
    text = text.lower()

    # Remove URLs
    text = re.sub(r'https?://\S+', '', text)

    # Remove new lines
    text = re.sub(r'[\r\n]+', ' ', text)

    # Remove mentions
    text = re.sub(r'@\w+', '', text)

    # Remove hashtags
    text = re.sub(r'#\w+', '', text)

    # Replace multiple spaces with single space
    text = re.sub(r'\s+', ' ', text)

    # Remove punctuation
    text = text.translate(str.maketrans('', '', string.punctuation))

    # Convert emojis to text
    text = emoji.demojize(text)

    return text.strip()

# Register cleaning UDF
pre_process_udf = udf(pre_process, StringType())

# Apply cleaning UDF
tweets_df = tweets_df.withColumn("clean_tweet", pre_process_udf(col("tweet")))
# Show cleaned tweets
tweets_df.select("tweet", "clean_tweet").show(5, truncate=False)

"""2.2 Tokenization"""

# Tokenization using RegexTokenizer (Spark native)
tokenizer = RegexTokenizer(inputCol="clean_tweet", outputCol="tokens", pattern="\\W")

tweets_df = tokenizer.transform(tweets_df)

# Show tokenized results
tweets_df.select("clean_tweet", "tokens").show(5, truncate=False)

# 4. Token counts (word count and vocabulary size)

# Explode tokens into individual words and filter out empty tokens
exploded_tokens_df = tweets_df.select(explode(col("tokens")).alias("token")).filter(col("token") != "")

# Count token frequencies
tokens_count_df = exploded_tokens_df.groupBy("token").count().orderBy("count", ascending=False)

# Calculate total word count and vocabulary size
total_word_count = exploded_tokens_df.count()
vocabulary_size = tokens_count_df.count()

print(f"Total Word Count\t: {total_word_count}")
print(f"Vocabulary Size\t\t: {vocabulary_size}")

tokens_count_df.show(10)

"""2.3 Stopwords Removal"""

import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

from pyspark.ml.feature import RegexTokenizer

# Step 1: Tokenize into 'new_tokens' to avoid conflict
tokenizer = RegexTokenizer(inputCol="clean_tweet", outputCol="new_tokens", pattern="\\W")
tweets_df = tokenizer.transform(tweets_df)

# Step 2: Drop old 'tokens' column (if exists), then rename 'new_tokens' to 'tokens'
tweets_df = tweets_df.drop("tokens").withColumnRenamed("new_tokens", "tokens")

# Optional: Show to confirm
tweets_df.select("clean_tweet", "tokens").show(5, truncate=False)

from pyspark.ml.feature import StopWordsRemover

# 1. Get default English stopwords from Spark
default_stopwords = StopWordsRemover.loadDefaultStopWords("english")

# 2. Customize stopwords list:
#    - Remove important negations to keep sentiment
important_words = {
    'not', 'no', 'nor', "don't", "doesn't", "didn't", "isn't", "wasn't",
    "weren't", "won't", "can't", "couldn't", "shouldn't", "wouldn't",
    "haven't", "hasn't", "hadn't", "ain", "aren", "needn", "shan", "mustn", "mightn", "wouldn"
}

# Remove important words from stopwords
filtered_stopwords = [w for w in default_stopwords if w not in important_words]

# Add 'im' to stopwords explicitly
filtered_stopwords.append('im')

# 3. Create StopWordsRemover with custom stopwords list
remover = StopWordsRemover(inputCol="tokens", outputCol="filtered_tokens", stopWords=filtered_stopwords)

# 4. Apply to your DataFrame
tweets_df = remover.transform(tweets_df)

# 5. Show results
tweets_df.select("tokens", "filtered_tokens").show(5, truncate=False)

"""6.Stemming"""

from pyspark.sql.functions import udf
from pyspark.sql.types import ArrayType, StringType
from nltk.stem import PorterStemmer

# 1. Define stemming function
def apply_stemming(tokens):
    stemmer = PorterStemmer()
    return [stemmer.stem(token) for token in tokens]

# 2. Create UDF
stemming_udf = udf(apply_stemming, ArrayType(StringType()))

# 3. Apply UDF to DataFrame
tweets_df = tweets_df.withColumn("stemmed_tokens", stemming_udf(col("filtered_tokens")))

# 4. Show before/after example
tweets_df.select("filtered_tokens", "stemmed_tokens").show(5, truncate=False)

"""8.Feature Extraction"""

from pyspark.ml.feature import Word2Vec

# 1. Initialize Word2Vec model
word2vec = Word2Vec(
    vectorSize=100,
    windowSize=5,
    minCount=5,
    inputCol="stemmed_tokens",
    outputCol="word2vec_features",
    seed=42,
    maxIter=10
)

# 2. Train model
w2v_model = word2vec.fit(tweets_df)

# 3. Transform the DataFrame to add word vector representations
tweets_df = w2v_model.transform(tweets_df)

# 4. Show results
tweets_df.select("stemmed_tokens", "word2vec_features").show(5, truncate=False)

"""7.Tokenizer and Sequence"""

from pyspark.sql.functions import udf
from pyspark.sql.types import StringType

sc = spark.sparkContext
join_tokens_udf = udf(lambda tokens: ' '.join(tokens), StringType())
tweets_df = tweets_df.withColumn("text_for_keras", join_tokens_udf("stemmed_tokens"))
# Collect all stemmed tokens into one flat list
all_tokens = tweets_df.select("stemmed_tokens").rdd.flatMap(lambda row: row[0]).collect()

# Build vocabulary
from collections import Counter
token_counts = Counter(all_tokens)
vocab = {word: idx + 1 for idx, (word, _) in enumerate(token_counts.most_common())}  # start indexing at 1
vocab_broadcast = sc.broadcast(vocab)
vocab_size = len(vocab) + 1

from pyspark.sql.types import ArrayType, IntegerType

def tokens_to_ids(tokens):
    return [vocab_broadcast.value.get(token, 0) for token in tokens]

tokens_to_ids_udf = udf(tokens_to_ids, ArrayType(IntegerType()))
tweets_df = tweets_df.withColumn("sequence", tokens_to_ids_udf("stemmed_tokens"))

def pad_sequence(seq, max_len=30):
    return seq[:max_len] + [0] * max(0, max_len - len(seq))

pad_udf = udf(lambda x: pad_sequence(x, 30), ArrayType(IntegerType()))
tweets_df = tweets_df.withColumn("padded_sequence", pad_udf("sequence"))

"""9.Embedding Matrix"""

import numpy as np

# 1. Get vectors from trained Word2Vec model
vectors_df = word2vec_model.getVectors()  # 'word' and 'vector' columns

# 2. Collect to driver as dictionary
vector_dict = {row['word']: row['vector'] for row in vectors_df.collect()}

# 3. Build word_index manually
word_index = {word: idx + 1 for idx, word in enumerate(vector_dict.keys())}  # reserve 0 for padding
vocab_size = len(word_index) + 1  # +1 for padding
embedding_dim = len(next(iter(vector_dict.values())))  # e.g., 100

# 4. Initialize embedding matrix
embedding_matrix = np.zeros((vocab_size, embedding_dim))

# 5. Fill embedding matrix
for word, i in word_index.items():
    embedding_matrix[i] = vector_dict[word]

# Optional: Check shape and a few rows
print("Embedding matrix shape:", embedding_matrix.shape)
print("Example row (word 'good'):", embedding_matrix[word_index.get('good', 0)])

"""10.Train with Embeddings"""

# After processing and building embedding matrix in PySpark:

# Collect tokens and labels to driver
token_lists = tweets_df.select('stemmed_tokens').rdd.map(lambda r: r[0]).collect()
labels = tweets_df.select('sentiment').rdd.map(lambda r: r[0]).collect()

# On driver, convert tokens to sequences (using your tokenizer from Keras or a manual map)
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical

# Prepare texts
texts = [' '.join(tokens) for tokens in token_lists]

tokenizer = Tokenizer()
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)

max_len = 30  # or as determined
X = pad_sequences(sequences, maxlen=max_len, padding='post')

# Encode labels
label_mapping = {'Positive':0, 'Neutral':1, 'Negative':2, 'Irrelevant':3}
y = to_categorical([label_mapping[label] for label in labels])

import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

# Build embedding matrix based on tokenizer.word_index and PySpark Word2Vec vectors
vocab_size = len(tokenizer.word_index) + 1
embedding_dim = 100  # from PySpark Word2Vec

embedding_matrix = np.zeros((vocab_size, embedding_dim))
for word, i in tokenizer.word_index.items():
    if word in vector_dict:
        embedding_matrix[i] = vector_dict[word]

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)

# Define model
model = Sequential([
    Embedding(input_dim=vocab_size, output_dim=embedding_dim, weights=[embedding_matrix], trainable=True),
    Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2)),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(4, activation='softmax')
])

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

# Train
history = model.fit(X_train, y_train, epochs=20, batch_size=64, validation_split=0.2)

# Save the model in HDF5 format
model.save('sentiment_lstm_model.h5')
print("Model saved successfully.")

from tensorflow.keras.models import load_model

# Load the model
model = load_model('sentiment_lstm_model.h5')
print("Model loaded successfully.")

import pickle

# Save tokenizer
with open('tokenizer.pkl', 'wb') as f:
    pickle.dump(tokenizer, f)

# Load tokenizer
with open('tokenizer.pkl', 'rb') as f:
    tokenizer = pickle.load(f)

# Plot training history (accuracy and loss)
plt.figure(figsize=(14, 5))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid()

plt.tight_layout()
plt.show()

"""13.Evaluation"""

loss, accuracy = model.evaluate(X_test, y_test)
print(f'Test Accuracy: {accuracy:.2f}')

"""11.Modeling Train a classifier

12.Model Building with Text Preprocessing
"""